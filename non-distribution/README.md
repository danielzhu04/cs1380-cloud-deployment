# M0: Setup & Centralized Computing

> Add your contact information below and in `package.json`.
* name: Grace Chen
* email: grace_a_chen@brown.edu
* cslogin: gchen76

## Summary

> Summarize your implementation, including the most challenging aspects; remember to update the `report` section of the `package.json` file with the total number of hours it took you to complete M0 (`hours`), the total number of JavaScript lines you added, including tests (`jsloc`), the total number of shell lines you added, including for deployment and testing (`sloc`).

My implementation consists of 6 components addressing T1--8. Among these components are c/stem.js (stems words, such as users' queried terms), c/getText.js (retrieves all text from an HTML page), c/getURLs.js (retrieves all URLs from a webpage), c/process.sh (processes text to get rid of non-letter characters, make all text lowercase, convert text into ASCII format, etc.), c/merge.js (merges incoming local index data into global index data), and query.js (when given a specific 1-, 2-, or 3-word input term, it retrieves global index data that contains the specified term). Overall, the different components make it so that when a user uses query.js to "search" for a term, this program models a search engine by searching among stored "web data" and returning the data entries that contain the searched term. Additionally, I wrote additional student tests across 9 files in the t/ts directory to test for implementation correctness (see the "Correctness & Performance Characterization" section for more details). 

The most challenging aspect of my implementation was just becoming familiarized with Javascript and Bash scripting. I found that I conceptually understood what the M0 handout was asking us to do, but I struggled to actually write an implementation due to having limited Javascript and Bash scripting experience. It took a lot of time to reference manuals and websites to grasp syntax rules and understand what specific modules/functions do, and thus, it took a lot of time to finish implementing the various components as well.

## Correctness & Performance Characterization

> Describe how you characterized the correctness and performance of your implementation.

To characterize correctness, I developed 15 tests across 9 files in the t/ts directory. These tests consist of...
1. 1 test for the c/stem.js component, which tests whether calling stem.js on a set of words outputs the stems of those words (e.g., some of the words will have their suffixes removed).
2. 6 tests for the query.js component, which test whether searching for 1-word, 2-word, and 3-word terms outputs the expected global index data entries, given a pre-specified global index file. 1 test also accounts for the case where the user provides a search term that doesn't match any global index data entries, in which case no data will be outputted. Another test accounts for the case where the user provides a search term that should be processed during the querying process (i.e., a search term with all capitalized letters should be made lowercase). The last test accounts for the case where the user provides a stopword as a search term, in which case the stopword should be removed during the querying process and all global index data should be returned (since the user would essentially be searching for nothing in particular). 
3. 1 test for the c/process.sh component, which tests that applying process.sh to text from a "webpage" (I created a file containing book titles to mimic the text contents of the "small collection of full books" testing corpora linked in the M0 handout) will result in a correctly processed result (e.g., all words become lowercase, non-letter characters are removed, etc.).
4. 1 test for the c/merge.js component, which tests that the component correctly "merges" pre-specified local index data with pre-specified global index data. In other words, local index data with terms not already contained in the global index data will be added as new entries to the global index data, while local index data with terms that are already contained in the global index data will be appended to existing global index data. Each modified existing global index data entry should maintain a proper format (i.e., always containing terms followed by a series of URLs and term-frequency-per-URL counts, with the series of URLs and frequencies sorted by frequency).
5. 1 test for the c/invert.sh component, which tests that calling the invert.sh script on a list of words (taken from the "small collection of full books" testing corpora) and providing a URL (I used the "small collection of full books" URL) will create a proper mapping from words to URLs and frequencies.
6. 1 test for the c/getURLs.js component, which tests that applying getURLs.js to an HTML page (I created a file containing HTML page data modeled after the "small collection of full books" website) outputs all the URLs contained in the page.
7. 1 test for the c/getText.js component, which tests that applying getText.js to an HTML page (the same HTML page I used for the aforementioned getURLs.js test) outputs page contents in text format.
8. 1 test for the c/combine.sh component, which tests that calling the combine.sh script on a set of words will create n-grams (where n=1,2,3) for those words.
9. 2 end-to-end tests, each testing that when the engine.sh script is run and a file containing a starting URL is provided, the d/visited.txt (which tracks visited URLs) and d/global-index.txt (which contains stored webpage term-to-URL-and-frequency data) files are correctly populated. 1 test utilizes a subpage linked in the "tiny web graph" testing corpora provided in the M0 handout, and the subpage contains a couple of hyperlinks. The other test uses another subpage linked in the "tiny web graph" testing corpora that doesn't contain any hyperlinks.

*Performance*: The throughput of various subsystems is described in the `"throughput"` portion of package.json. The characteristics of my development machines are summarized in the `"dev"` portion of package.json.

The two lists of throughput values in package.json corresponding to the "dev" and "aws" fields under the "throughput" portion each contain three values -- the crawler throughput, the indexer throughput, and the query throughput, in that order. 

The corpora I used to calculate throughput was the "tiny web graph" (sandbox 1) linked in the M0 handout and located at https://cs.brown.edu/courses/csci1380/sandbox/1/. I tested my program's performance both locally and on the cloud. 

I used the "time ./crawl.sh https://cs.brown.edu/courses/csci1380/sandbox/1 >d/content.txt" command to help measure the crawler's throughput. Each time the command is run, it returns the seconds the command took to run in "real," "user," and "sys" time measurements. I ran the aforementioned command 10 times, noting down the "real" time measurement (in seconds) for all 10 trials. Each time the aforementioned command was run, the crawler visited and downloaded page contents for only 1 page (the page at https://cs.brown.edu/courses/csci1380/sandbox/1). Therefore, to get the crawler's throughput, I divided the number 10 (10 pages downloaded in total throughout the 10 trials) by the sum of my 10 time measurements (the total time elapsed in seconds). This gave me a final value of 1.906 pages downloaded per second locally and 0.808 pages downloaded per second on the cloud. 

Similarly, I used the "time ./index.sh d/content.txt https://cs.brown.edu/courses/csci1380/sandbox/1" command to help measure the indexer's throughput. I ran the aforementioned command 10 times once again. Each time I noted down the outputted "real" time (in seconds) it took for the command to finish executing. The aforementioned indexer command only indexes 1 page (the page at https://cs.brown.edu/courses/csci1380/sandbox/1). Therefore, to get the indexer's throughput, I divided the number 10 (10 pages indexed in total across my 10 trials) by the sum of my 10 time measurements. This gave me a final value of 3.484 pages indexed per second locally and 0.979 pages indexed per second on the cloud. 

Lastly, I used the "time ./query.js (query terms here)" command to help measure my query throughput. Since running the aforementioned indexer command beforehand would've already populated the global index data stored in d/global-index.txt, I simply ran the "time ./query.js (query terms here)" command 10 times using both query terms found in the global index data (e.g., "stuff" and "link check") as well as query terms not found in the global index data. I made sure to use terms made up of 1, 2, and 3 words. To get the query throughput, I divided the number 10 (10 queries made in total, 1 query per trial) by the sum of my 10 time measurements. This gave me a final value of 3.139 queries per second locally and 1.103 queries per second on the cloud.

## Wild Guess

> How many lines of code do you think it will take to build the fully distributed, scalable version of your search engine? Add that number to the `"dloc"` portion of package.json, and justify your answer below.

I wrote the number 16,000 in the "dloc" portion of the package.json file. Each of the 7 files in the c directory as well as the query.js file contain, on average, around 50 lines of code (a very rough estimate). Therefore, most of the functionality for this program is contained in roughly 400 lines of code (50 lines x 8 files). I think that a fully distributed, scalable version of this search engine would likely contain many, *many* more files and features than this current search engine due to how much more complex distributed systems tend to be compared to non-distributed systems in general (in order to ensure proper failure tolerance, scalability, etc.), which I estimate would translate to roughly 40x more lines of code (400 lines of code x 40 = 16,000).
